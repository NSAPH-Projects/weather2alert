import os
import pandas as pd
import hydra


configfile: "conf/snakemake.yaml"  # Path: conf/config.yaml


conda: "requirements.yaml"  # Path: envs/conda.yaml


envvars:
    "CENSUS_API_KEY",


# read hydra config from initialize API
with hydra.initialize(config_path="conf", version_base=None):
    # here we set the data path for real hospitazation training data
    processing_cfg = hydra.compose(config_name="config")


# make sure data locations exist / user can set them to
# symlinks separately if preferred
data_dir = processing_cfg.data_dir
os.makedirs(f"{data_dir}/raw", exist_ok=True)
os.makedirs(f"{data_dir}/processed", exist_ok=True)

splits = ["65k", "all"]


rule all:
    input:
        expand(
            data_dir + "/processed/{split}/endogenous_states_actions.parquet",
            split=splits,
        ),
        expand(
            data_dir + "/processed/{split}/exogenous_states.parquet",
            split=splits,
        ),
        expand(
            data_dir + "/processed/{split}/budget.parquet",
            split=splits,
        ),
        expand(
            data_dir + "/processed/{split}/confounders.parquet",
            split=splits,
        ),
        data_dir + "/processed/bspline_basis.parquet",


rule merge_state_actions:
    input:
        expand(
            data_dir + "/processed/alerts/{state}.parquet",
            state=config["states"],
        ),
        expand(
            data_dir + "/processed/{split}/heatmetrics.parquet",
            split=splits,
        ),
    output:
        expand(
            data_dir + "/processed/{split}/endogenous_states_actions.parquet",
            split=splits,
        ),
        expand(
            data_dir + "/processed/{split}/exogenous_states.parquet",
            split=splits,
        ),
        expand(
            data_dir + "/processed/{split}/budget.parquet",
            split=splits,
        ),
        data_dir + "/processed/bspline_basis.parquet",
    log:
        "logs/merge_state_actions.log",
    shell:
        "python merge_state_actions.py &> {log}"


rule confounders:
    output:
        expand(
            data_dir + "/processed/{split}/confounders.parquet",
            split=splits,
        ),
    log:
        "logs/confounders.log",
    shell:
        f"""
        python confounders.py census_api_key={os.environ['CENSUS_API_KEY']} &> {{log}}
        """


rule heatmetrics:
    input:
        expand(
            data_dir + "/processed/{split}/confounders.parquet",
            split=splits,
        ),
    output:
        expand(
            data_dir + "/processed/{split}/heatmetrics.parquet",
            split=splits,
        ),
    log:
        "logs/heatmetrics.log",
    shell:
        "python heatmetrics.py &> {log}"


rule alerts:
    output:
        data_dir + "/processed/alerts/{state}.parquet",
    log:
        "logs/alerts_{state}.log",
    shell:
        "python heatalerts.py alerts.state={wildcards.state} &> {log}"
